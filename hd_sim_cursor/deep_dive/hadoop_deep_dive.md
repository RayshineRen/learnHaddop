# Hadoop 深度剖析：MapReduce、Shuffle、YARN 与 Spark 迁移

本文档从四个维度深入剖析 Hadoop 核心机制，帮助你建立扎实的分布式计算直觉。

---

## 目录

1. [强化 MapReduce 直觉：单个 Map Task 的视角](#一强化-mapreduce-直觉单个-map-task-的视角)
2. [深挖 Shuffle：MapReduce 的性能瓶颈](#二深挖-shufflemapreduce-的性能瓶颈)
3. [YARN 调度视角：Container 分配全过程](#三yarn-调度视角container-分配全过程)
4. [迁移到 Spark/Flink：什么变了，什么没变](#四迁移到-sparkflink什么变了什么没变)

---

## 一、强化 MapReduce 直觉：单个 Map Task 的视角

让我们换一个角度——**假设你就是一个 Map Task**，你被 YARN 调度器唤醒，准备开始工作。

### 1.1 启动时，Map Task 拿到了哪些信息？

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    Map Task 启动时的"工作包"                            │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  当 JVM 启动、MapTask 类被实例化时，它会收到一个 TaskAttemptContext，   │
│  其中包含以下关键信息：                                                 │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  1. InputSplit 信息                                             │   │
│  │     ├─ 文件路径: hdfs://namenode:9000/data/logs/part-00001     │   │
│  │     ├─ 起始偏移量: 134217728 (第 128MB 开始)                    │   │
│  │     ├─ 长度: 134217728 (128MB)                                  │   │
│  │     └─ 数据位置: ["datanode-1", "datanode-3", "datanode-5"]    │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  2. 作业配置 (Job Configuration)                                │   │
│  │     ├─ Mapper 类: com.example.PVUVMapper                       │   │
│  │     ├─ 输出 Key 类型: Text                                      │   │
│  │     ├─ 输出 Value 类型: Text                                    │   │
│  │     ├─ Partitioner 类: HashPartitioner                         │   │
│  │     ├─ Reducer 数量: 3  ← 重要！决定分区数                      │   │
│  │     ├─ Combiner 类: (可选) PVUVCombiner                        │   │
│  │     └─ 各种调优参数...                                          │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  3. 任务元数据                                                  │   │
│  │     ├─ Task ID: attempt_202401201000_0001_m_000003_0           │   │
│  │     ├─ Job ID: job_202401201000_0001                           │   │
│  │     └─ 本地工作目录: /tmp/hadoop-yarn/nm-local-dir/usercache/  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

**关键洞察**：
- Map Task **知道 Reducer 的数量**，但**不知道 Reducer 在哪里运行**
- Map Task **不与其他 Map Task 通信**，它们是完全独立的
- InputSplit 的 `locations` 字段只是**建议**，不是强制要求

### 1.2 Map Task 读了 HDFS 的哪些 Block？

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    Map Task 读取数据的过程                              │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【场景】InputSplit 指定读取 part-00001 的第 128MB-256MB                │
│                                                                         │
│  【问题】Block 边界和行边界不一致怎么办？                               │
│                                                                         │
│       Block 1 (128MB)          Block 2 (128MB)                         │
│       ├────────────────────────┼────────────────────────┤              │
│       │...日志行 A\n日志行 B\n日│志行 C\n日志行 D\n...   │              │
│       └────────────────────────┴────────────────────────┘              │
│                                ↑                                        │
│                           Block 边界                                    │
│                           (字节偏移 134217728)                          │
│                                                                         │
│  【TextInputFormat 的处理策略】                                         │
│                                                                         │
│  Map Task 0 (负责 Block 1):                                            │
│    1. 从 offset=0 开始读取                                             │
│    2. 读到 Block 边界时，**继续往后读**直到遇到 \n                      │
│    3. 所以它会读取: 日志行 A, 日志行 B, 日志行 C (跨 Block!)           │
│                                                                         │
│  Map Task 1 (负责 Block 2):                                            │
│    1. 从 offset=134217728 开始                                         │
│    2. **跳过第一个不完整的行** (日志行 C 的后半部分)                    │
│    3. 从下一个 \n 之后开始读取: 日志行 D, ...                          │
│                                                                         │
│  【结果】每条日志**恰好被一个 Map Task 处理**，不重不漏               │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

**代码层面发生了什么**：

```java
// LineRecordReader.java 核心逻辑（简化版）
public void initialize(InputSplit split, TaskAttemptContext context) {
    FileSplit fileSplit = (FileSplit) split;
    start = fileSplit.getStart();           // 起始偏移量
    end = start + fileSplit.getLength();    // 结束偏移量
    
    // 打开 HDFS 文件流，定位到 start
    FSDataInputStream fileIn = fs.open(fileSplit.getPath());
    fileIn.seek(start);
    
    // 关键：如果不是文件开头，跳过第一个不完整的行
    if (start != 0) {
        start += readUntilNewline(fileIn);  // 跳到下一行
    }
}

public boolean nextKeyValue() {
    // 读取一行，key=行偏移量，value=行内容
    // 即使超过 end，也会读完整行
    ...
}
```

### 1.3 Map Task 是否知道 Reducer 的存在？

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    Map Task 对 Reducer 的"认知"                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  Map Task 知道的：                                                      │
│  ────────────────                                                       │
│  ✓ Reducer 的数量 (mapreduce.job.reduces = 3)                          │
│  ✓ Partitioner 类 (决定 key 去哪个 Reducer)                            │
│  ✓ 需要为每个 Reducer 准备一个分区的输出                                │
│                                                                         │
│  Map Task 不知道的：                                                    │
│  ────────────────                                                       │
│  ✗ Reducer 运行在哪些节点上                                            │
│  ✗ Reducer 什么时候启动                                                │
│  ✗ 其他 Map Task 的存在和进度                                          │
│                                                                         │
│  【设计哲学】                                                           │
│  Map Task 不需要知道 Reducer 在哪里！它只需要：                         │
│  1. 把输出按 Partition 分好                                            │
│  2. 写到本地磁盘                                                        │
│  3. 向 ApplicationMaster 报告 "我完成了，输出在这里"                   │
│                                                                         │
│  Reducer 会主动来 **拉取** (Pull) 数据，而不是 Map 主动推送             │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 1.4 Map Task 如何决定输出发往哪个 Reducer？

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    分区决策过程                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【时机】每次 context.write(key, value) 调用时                          │
│                                                                         │
│  【步骤】                                                               │
│                                                                         │
│  mapper.map(key, value, context)                                       │
│       │                                                                │
│       ▼                                                                │
│  context.write(outputKey, outputValue)                                 │
│       │                                                                │
│       ▼                                                                │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  int partition = partitioner.getPartition(                      │   │
│  │      outputKey,                                                 │   │
│  │      outputValue,                                               │   │
│  │      numReducers    // = 3                                      │   │
│  │  );                                                             │   │
│  │                                                                 │   │
│  │  // 默认 HashPartitioner:                                       │   │
│  │  // return (key.hashCode() & Integer.MAX_VALUE) % numReducers;  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│       │                                                                │
│       ▼                                                                │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  写入环形缓冲区 (默认 100MB)                                    │   │
│  │  每条记录携带: (partition, key, value)                          │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  【示例】                                                               │
│                                                                         │
│  假设 numReducers = 3：                                                │
│                                                                         │
│  key = "/product/phone"                                                │
│  hashCode = 1847329054                                                 │
│  partition = (1847329054 & 0x7FFFFFFF) % 3 = 1                        │
│  → 这条记录将被 Reducer 1 处理                                        │
│                                                                         │
│  key = "/product/laptop"                                               │
│  hashCode = -893274621                                                 │
│  partition = ((-893274621) & 0x7FFFFFFF) % 3 = 2                      │
│  → 这条记录将被 Reducer 2 处理                                        │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

**关键点总结**：

| 问题 | 答案 |
|------|------|
| 分区决策发生在哪？ | Map Task 内部，每次 write() 时 |
| 谁决定分区？ | Partitioner 类（默认 HashPartitioner） |
| 分区依据是什么？ | Key 的 hashCode() 对 Reducer 数量取模 |
| 分区信息存在哪？ | 随记录一起写入 Map 输出文件 |

---

## 二、深挖 Shuffle：MapReduce 的性能瓶颈

Shuffle 是 MapReduce 最复杂、最容易出性能问题的阶段。让我们拆解它。

### 2.1 Map 端 Shuffle：发生了什么？

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         Map 端 Shuffle 详解                             │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【阶段1】写入环形缓冲区 (In-Memory Buffer)                             │
│  ─────────────────────────────────────────                              │
│                                                                         │
│  ┌───────────────────────────────────────────────────────────────┐     │
│  │                    环形缓冲区 (100MB 默认)                     │     │
│  │                                                               │     │
│  │   ←── 数据写入方向 ───    ←── 索引写入方向 ───                │     │
│  │   ┌────────────────────────────────────────────────────┐     │     │
│  │   │ 数据区域                    │   │   │ 索引区域     │     │     │
│  │   │ (key, value 序列化后)       │   │   │ (partition,  │     │     │
│  │   │                             │ ← │ → │  keyStart,   │     │     │
│  │   │                             │   │   │  valueStart) │     │     │
│  │   └────────────────────────────────────────────────────┘     │     │
│  │                                  ↑                           │     │
│  │                              赤道线                           │     │
│  │                                                               │     │
│  │   当使用量达到 80% (spill.percent) 时，触发溢写               │     │
│  └───────────────────────────────────────────────────────────────┘     │
│                                                                         │
│  【阶段2】溢写 (Spill) - 后台线程执行                                   │
│  ─────────────────────────────────────                                  │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  2.1 按 Partition 排序（快速排序）                              │   │
│  │      排序结果: P0 的记录 | P1 的记录 | P2 的记录 | ...          │   │
│  │                                                                 │   │
│  │  2.2 如果配置了 Combiner，对每个 Partition 执行本地聚合        │   │
│  │      例如: (url, 1), (url, 1), (url, 1) → (url, 3)             │   │
│  │                                                                 │   │
│  │  2.3 写入本地磁盘文件                                          │   │
│  │      文件格式: spill0.out, spill1.out, ...                     │   │
│  │      每个文件包含所有 Partition 的数据（但分区内有序）          │   │
│  │                                                                 │   │
│  │  2.4 同时生成索引文件                                          │   │
│  │      记录每个 Partition 在文件中的偏移量和长度                  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  【阶段3】合并 (Merge) - Map Task 结束前                                │
│  ─────────────────────────────────────────                              │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                                                                 │   │
│  │  spill0.out ─┐                                                  │   │
│  │  spill1.out ─┼──→ 多路归并排序 ──→ file.out (最终输出)         │   │
│  │  spill2.out ─┘              │                                   │   │
│  │                             ▼                                   │   │
│  │                       file.out.index (索引)                     │   │
│  │                                                                 │   │
│  │  合并后，每个 Partition 的数据是连续的、按 Key 排序的           │   │
│  │                                                                 │   │
│  │  ┌──────────────────────────────────────────────────────────┐  │   │
│  │  │  file.out 结构:                                          │  │   │
│  │  │  [Partition 0 数据] [Partition 1 数据] [Partition 2 数据]│  │   │
│  │  │  ↑                  ↑                  ↑                 │  │   │
│  │  │  索引记录的偏移量                                        │  │   │
│  │  └──────────────────────────────────────────────────────────┘  │   │
│  │                                                                 │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 2.2 Reduce 端什么时候开始拉数据？

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    Reduce 端数据拉取时机                                │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【关键参数】mapreduce.job.reduce.slowstart.completedmaps              │
│  默认值: 0.05 (5%)                                                     │
│                                                                         │
│  【含义】                                                               │
│  当 5% 的 Map Task 完成后，Reduce Task 就开始启动并拉取数据             │
│                                                                         │
│  【时间线】                                                             │
│                                                                         │
│  Map Tasks:                                                            │
│  ────────────────────────────────────────────────────────────────▶ 时间│
│  Map-0 ████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ (完成)         │
│  Map-1 ████████████████████████░░░░░░░░░░░░░░░░░░░░░░░ (完成)         │
│  Map-2 ████████████████████████████████░░░░░░░░░░░░░░░ (完成)         │
│  Map-3 ████████████████████████████████████████████████ (完成)         │
│         ↑                                                              │
│         5% Map 完成                                                    │
│         Reduce 开始启动                                                │
│                                                                         │
│  Reduce Tasks:                                                         │
│  ────────────────────────────────────────────────────────────────▶ 时间│
│  Reduce-0       ░░░░████████████████████████████████████████           │
│  Reduce-1       ░░░░██████████████████████████████████████████████     │
│                 │   │                   │                              │
│                 │   └── Copy 阶段       └── Reduce 阶段                │
│                 └────── 等待启动                                       │
│                                                                         │
│  【Copy 阶段详解】                                                      │
│                                                                         │
│  Reduce-0 的视角：                                                     │
│  1. 询问 ApplicationMaster: "哪些 Map 已经完成？"                      │
│  2. AM 返回: "Map-0 完成了，输出在 node-1:/tmp/.../file.out"          │
│  3. Reduce-0 向 node-1 发起 HTTP 请求，拉取 Partition-0 的数据         │
│  4. 重复，直到所有 Map 都完成                                          │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  // Reduce 端并行拉取数据                                       │   │
│  │  // 默认 5 个线程同时从不同 Map 节点拉取                        │   │
│  │  // 参数: mapreduce.reduce.shuffle.parallelcopies = 5          │   │
│  │                                                                 │   │
│  │  Fetcher Thread 0 ──→ 从 Map-0 (node-1) 拉取                   │   │
│  │  Fetcher Thread 1 ──→ 从 Map-1 (node-2) 拉取                   │   │
│  │  Fetcher Thread 2 ──→ 从 Map-3 (node-1) 拉取                   │   │
│  │  Fetcher Thread 3 ──→ 等待 Map-2 完成...                       │   │
│  │  Fetcher Thread 4 ──→ 等待分配任务...                          │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 2.3 异常场景分析

#### 场景1：Map 很慢（长尾效应）

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    Map 长尾效应                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【场景】99 个 Map 都完成了，1 个 Map 还在跑                            │
│                                                                         │
│  Map Tasks:                                                            │
│  Map-0   ████ (完成)                                                   │
│  Map-1   ████ (完成)                                                   │
│  ...                                                                    │
│  Map-98  ████ (完成)                                                   │
│  Map-99  ████████████████████████████████████████████████ (还在跑！)   │
│                                                                         │
│  Reduce Tasks:                                                         │
│  Reduce-0  ████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░            │
│            │                  │                                         │
│            已拉取 99 个 Map   └── 等待 Map-99...                        │
│                                                                         │
│  【后果】                                                               │
│  1. Reduce 必须等待所有 Map 完成才能开始最终聚合                        │
│  2. 整个作业被一个慢 Map 拖累                                          │
│  3. 资源利用率低（其他 Reduce 在空等）                                  │
│                                                                         │
│  【解决方案】                                                           │
│  1. 推测执行 (Speculative Execution)                                   │
│     - mapreduce.map.speculative = true                                 │
│     - 检测到慢任务时，在另一个节点启动备份任务                          │
│     - 谁先完成用谁的结果                                               │
│                                                                         │
│  2. 分析根因                                                           │
│     - 数据倾斜？某个 Split 特别大？                                    │
│     - 节点故障？GC 频繁？网络问题？                                    │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 场景2：Reduce 很慢

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    Reduce 慢的几种情况                                  │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【情况1】Copy 阶段慢 - 网络瓶颈                                       │
│  ─────────────────────────────────────                                  │
│                                                                         │
│  症状: Reduce 进度一直卡在 33%（Copy 阶段占 1/3 进度）                  │
│  原因:                                                                  │
│    - 跨机架传输大量数据                                                 │
│    - Map 输出节点磁盘 I/O 瓶颈                                         │
│    - 网络带宽不足                                                       │
│                                                                         │
│  解决:                                                                  │
│    - 使用 Combiner 减少 Shuffle 数据量                                 │
│    - 增加 Reduce 并行拉取线程数                                        │
│    - 优化数据本地性                                                     │
│                                                                         │
│  【情况2】Sort/Merge 阶段慢 - 内存不足                                 │
│  ─────────────────────────────────────────                              │
│                                                                         │
│  症状: Reduce 进度卡在 67% 附近                                        │
│  原因:                                                                  │
│    - 拉取的数据太多，内存放不下                                         │
│    - 频繁溢写到磁盘，触发多次合并                                       │
│                                                                         │
│  解决:                                                                  │
│    - 增加 mapreduce.reduce.shuffle.memory.percent                     │
│    - 增加 Reducer 数量，减少每个 Reducer 处理的数据量                  │
│                                                                         │
│  【情况3】Reduce 计算慢 - 数据倾斜                                     │
│  ───────────────────────────────────────                                │
│                                                                         │
│  症状: 某个 Reduce 卡在 67%~100%，其他都完成了                         │
│  原因:                                                                  │
│    - 某个 Key 的 Values 特别多（例如热门商品）                         │
│    - 单个 Reducer 处理的数据量远超其他                                  │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  示例：数据倾斜                                                 │   │
│  │                                                                 │   │
│  │  Reducer-0: /product/normal-item  → 1,000 条记录 ✓             │   │
│  │  Reducer-1: /product/hot-item     → 10,000,000 条记录 ✗ 慢！   │   │
│  │  Reducer-2: /product/another-item → 2,000 条记录 ✓             │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  解决:                                                                  │
│    - 加盐打散: 把热门 Key 拆成多个 (hot-item_0, hot-item_1, ...)      │
│    - 两阶段聚合: 先局部聚合，再全局聚合                                 │
│    - 单独处理: 把倾斜 Key 单独拎出来处理                               │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 2.4 为什么 Shuffle 是性能瓶颈？

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    Shuffle 性能瓶颈分析                                 │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【瓶颈1】磁盘 I/O                                                      │
│  ──────────────────                                                     │
│                                                                         │
│  Map 端:                                                               │
│    写缓冲区 → 溢写到磁盘 → 多次合并                                    │
│    每条记录可能被写入磁盘 3 次以上！                                    │
│                                                                         │
│  Reduce 端:                                                            │
│    网络接收 → 写磁盘 → 合并排序 → 读出来处理                           │
│    又是多次磁盘 I/O                                                    │
│                                                                         │
│  【瓶颈2】网络传输                                                      │
│  ──────────────────                                                     │
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                                                                 │   │
│  │   假设: 100 个 Map, 10 个 Reduce, 每个 Map 输出 1GB            │   │
│  │                                                                 │   │
│  │   网络传输总量 = 100GB × 10 = 1TB !!!                          │   │
│  │                                                                 │   │
│  │   （实际上每个 Reduce 只取 1/10，但要从所有 Map 都拉一次）      │   │
│  │                                                                 │   │
│  │   每个 Reduce 需要:                                             │   │
│  │     - 连接 100 个 Map 节点                                      │   │
│  │     - 下载 100 × 100MB = 10GB 数据                              │   │
│  │                                                                 │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  【瓶颈3】序列化/反序列化                                               │
│  ──────────────────────────                                             │
│                                                                         │
│  数据在整个过程中被序列化/反序列化多次:                                 │
│    1. Map 输出: Java 对象 → 字节                                       │
│    2. Reduce 输入: 字节 → Java 对象                                    │
│    3. 如果有 Combiner，还要反序列化再序列化                             │
│                                                                         │
│  【瓶颈4】内存压力                                                      │
│  ─────────────────                                                      │
│                                                                         │
│  环形缓冲区、Shuffle 缓冲区都需要内存                                   │
│  内存不足 → 频繁溢写 → 更多磁盘 I/O → 恶性循环                         │
│                                                                         │
│  ════════════════════════════════════════════════════════════════════  │
│                                                                         │
│  【总结】Shuffle 是瓶颈的根本原因:                                      │
│                                                                         │
│  ┌────────────────────────────────────────────────────────┐            │
│  │                                                        │            │
│  │   Shuffle 涉及大量的：                                 │            │
│  │     • 磁盘随机写入                                     │            │
│  │     • 磁盘随机读取                                     │            │
│  │     • 跨网络数据传输                                   │            │
│  │     • 排序和合并操作                                   │            │
│  │                                                        │            │
│  │   这些都是分布式系统中最慢的操作！                     │            │
│  │                                                        │            │
│  └────────────────────────────────────────────────────────┘            │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 三、YARN 调度视角：Container 分配全过程

假设我们的电商日志分析作业需要在一个 3 节点集群上运行。以下用"调度日志"形式展示 YARN 的工作过程。

### 3.1 集群环境

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           集群配置                                      │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ResourceManager: rm-master (192.168.1.100)                            │
│                                                                         │
│  NodeManager 列表:                                                      │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  NodeManager-1 (node-1, 192.168.1.101)                          │   │
│  │    - 总资源: 8 vCores, 16384 MB                                 │   │
│  │    - HDFS 数据: Block-0, Block-3, Block-6                       │   │
│  ├─────────────────────────────────────────────────────────────────┤   │
│  │  NodeManager-2 (node-2, 192.168.1.102)                          │   │
│  │    - 总资源: 8 vCores, 16384 MB                                 │   │
│  │    - HDFS 数据: Block-1, Block-4, Block-7                       │   │
│  ├─────────────────────────────────────────────────────────────────┤   │
│  │  NodeManager-3 (node-3, 192.168.1.103)                          │   │
│  │    - 总资源: 8 vCores, 16384 MB                                 │   │
│  │    - HDFS 数据: Block-2, Block-5, Block-8                       │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  作业需求:                                                              │
│    - 9 个 Map Task (每个处理 1 个 Block)                               │
│    - 3 个 Reduce Task                                                  │
│    - Map 资源: 1 vCore, 1024 MB                                        │
│    - Reduce 资源: 1 vCore, 2048 MB                                     │
│    - AM 资源: 1 vCore, 1536 MB                                         │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 3.2 调度日志

```
╔═════════════════════════════════════════════════════════════════════════╗
║                        YARN 调度日志                                    ║
╠═════════════════════════════════════════════════════════════════════════╣
║                                                                         ║
║  [10:00:00.000] ──────────────────────────────────────────────────────  ║
║  │ EVENT: 客户端提交作业                                               ║
║  │ ACTION: Client → ResourceManager                                    ║
║  │ DETAIL: 提交 application_20240120_0001 (PV/UV 统计作业)            ║
║  │ PAYLOAD:                                                            ║
║  │   - InputSplits: 9 个                                               ║
║  │   - 请求 Map 资源: 9 × (1 vCore, 1024 MB)                          ║
║  │   - 请求 Reduce 资源: 3 × (1 vCore, 2048 MB)                       ║
║  └─────────────────────────────────────────────────────────────────────  ║
║                                                                         ║
║  [10:00:00.050] ──────────────────────────────────────────────────────  ║
║  │ EVENT: ResourceManager 分配 ApplicationMaster Container             ║
║  │ DECISION: 选择 node-1 (当前负载最低)                                ║
║  │ RESULT:                                                             ║
║  │   ┌─────────────────────────────────────────────────────────────┐   ║
║  │   │  Container ID: container_20240120_0001_01_000001            │   ║
║  │   │  Node: node-1                                                │   ║
║  │   │  Resource: 1 vCore, 1536 MB                                  │   ║
║  │   │  Purpose: ApplicationMaster                                  │   ║
║  │   └─────────────────────────────────────────────────────────────┘   ║
║  │                                                                     ║
║  │ NODE STATUS after allocation:                                       ║
║  │   node-1: 7 vCores free, 14848 MB free                             ║
║  │   node-2: 8 vCores free, 16384 MB free                             ║
║  │   node-3: 8 vCores free, 16384 MB free                             ║
║  └─────────────────────────────────────────────────────────────────────  ║
║                                                                         ║
║  [10:00:00.200] ──────────────────────────────────────────────────────  ║
║  │ EVENT: ApplicationMaster 启动完成                                   ║
║  │ ACTION: AM 向 RM 注册，开始申请 Map Container                      ║
║  │ REQUEST:                                                            ║
║  │   Map-0: prefer [node-1] (Block-0 在 node-1)                       ║
║  │   Map-1: prefer [node-2] (Block-1 在 node-2)                       ║
║  │   Map-2: prefer [node-3] (Block-2 在 node-3)                       ║
║  │   Map-3: prefer [node-1] (Block-3 在 node-1)                       ║
║  │   Map-4: prefer [node-2] (Block-4 在 node-2)                       ║
║  │   Map-5: prefer [node-3] (Block-5 在 node-3)                       ║
║  │   Map-6: prefer [node-1] (Block-6 在 node-1)                       ║
║  │   Map-7: prefer [node-2] (Block-7 在 node-2)                       ║
║  │   Map-8: prefer [node-3] (Block-8 在 node-3)                       ║
║  └─────────────────────────────────────────────────────────────────────  ║
║                                                                         ║
║  [10:00:00.250] ──────────────────────────────────────────────────────  ║
║  │ EVENT: ResourceManager 调度周期 #1                                  ║
║  │ SCHEDULER: FairScheduler                                           ║
║  │                                                                     ║
║  │ ALLOCATION DECISIONS:                                               ║
║  │                                                                     ║
║  │ ✓ Map-0 → node-1 (NODE_LOCAL, Block-0 就在这里)                    ║
║  │   container_20240120_0001_01_000002                                 ║
║  │                                                                     ║
║  │ ✓ Map-1 → node-2 (NODE_LOCAL, Block-1 就在这里)                    ║
║  │   container_20240120_0001_01_000003                                 ║
║  │                                                                     ║
║  │ ✓ Map-2 → node-3 (NODE_LOCAL, Block-2 就在这里)                    ║
║  │   container_20240120_0001_01_000004                                 ║
║  │                                                                     ║
║  │ ✓ Map-3 → node-1 (NODE_LOCAL, Block-3 就在这里)                    ║
║  │   container_20240120_0001_01_000005                                 ║
║  │                                                                     ║
║  │ ✓ Map-4 → node-2 (NODE_LOCAL, Block-4 就在这里)                    ║
║  │   container_20240120_0001_01_000006                                 ║
║  │                                                                     ║
║  │ ✓ Map-5 → node-3 (NODE_LOCAL, Block-5 就在这里)                    ║
║  │   container_20240120_0001_01_000007                                 ║
║  │                                                                     ║
║  │ NODE STATUS after allocation:                                       ║
║  │   node-1: 5 vCores free, 12800 MB free (AM + Map-0 + Map-3)        ║
║  │   node-2: 6 vCores free, 14336 MB free (Map-1 + Map-4)             ║
║  │   node-3: 6 vCores free, 14336 MB free (Map-2 + Map-5)             ║
║  └─────────────────────────────────────────────────────────────────────  ║
║                                                                         ║
║  [10:00:00.300] ──────────────────────────────────────────────────────  ║
║  │ EVENT: ResourceManager 调度周期 #2                                  ║
║  │                                                                     ║
║  │ ALLOCATION DECISIONS:                                               ║
║  │                                                                     ║
║  │ ✓ Map-6 → node-1 (NODE_LOCAL, Block-6 就在这里)                    ║
║  │   container_20240120_0001_01_000008                                 ║
║  │                                                                     ║
║  │ ✓ Map-7 → node-2 (NODE_LOCAL, Block-7 就在这里)                    ║
║  │   container_20240120_0001_01_000009                                 ║
║  │                                                                     ║
║  │ ✓ Map-8 → node-3 (NODE_LOCAL, Block-8 就在这里)                    ║
║  │   container_20240120_0001_01_000010                                 ║
║  │                                                                     ║
║  │ 🎯 所有 9 个 Map Task 都实现了 NODE_LOCAL 调度！                    ║
║  │                                                                     ║
║  │ NODE STATUS after allocation:                                       ║
║  │   node-1: 4 vCores free, 11776 MB free (AM + 3 Maps)               ║
║  │   node-2: 5 vCores free, 13312 MB free (3 Maps)                    ║
║  │   node-3: 5 vCores free, 13312 MB free (3 Maps)                    ║
║  └─────────────────────────────────────────────────────────────────────  ║
║                                                                         ║
║  [10:00:05.000] ──────────────────────────────────────────────────────  ║
║  │ EVENT: Map-0, Map-1, Map-2 完成 (5% Map 完成)                      ║
║  │ ACTION: AM 开始申请 Reduce Container                               ║
║  │ REQUEST:                                                            ║
║  │   Reduce-0: any node, 1 vCore, 2048 MB                             ║
║  │   Reduce-1: any node, 1 vCore, 2048 MB                             ║
║  │   Reduce-2: any node, 1 vCore, 2048 MB                             ║
║  └─────────────────────────────────────────────────────────────────────  ║
║                                                                         ║
║  [10:00:05.100] ──────────────────────────────────────────────────────  ║
║  │ EVENT: ResourceManager 调度周期 #N                                  ║
║  │                                                                     ║
║  │ ALLOCATION DECISIONS (Reduce):                                      ║
║  │                                                                     ║
║  │ 注意: Reduce 没有数据本地性要求，均匀分配到各节点                   ║
║  │                                                                     ║
║  │ ✓ Reduce-0 → node-2 (负载相对较低)                                 ║
║  │   container_20240120_0001_01_000011                                 ║
║  │                                                                     ║
║  │ ✓ Reduce-1 → node-3 (负载相对较低)                                 ║
║  │   container_20240120_0001_01_000012                                 ║
║  │                                                                     ║
║  │ ✓ Reduce-2 → node-1 (释放了完成的 Map Container 后)               ║
║  │   container_20240120_0001_01_000013                                 ║
║  │                                                                     ║
║  │ NODE STATUS:                                                        ║
║  │   node-1: 5 vCores free, 12800 MB free (AM + 2 Maps + 1 Reduce)   ║
║  │   node-2: 4 vCores free, 11264 MB free (3 Maps + 1 Reduce)        ║
║  │   node-3: 4 vCores free, 11264 MB free (3 Maps + 1 Reduce)        ║
║  └─────────────────────────────────────────────────────────────────────  ║
║                                                                         ║
║  [10:00:15.000] ──────────────────────────────────────────────────────  ║
║  │ EVENT: 所有 Map Task 完成                                          ║
║  │ STATUS:                                                             ║
║  │   Map: 9/9 完成 (100%)                                             ║
║  │   Reduce: 0/3 完成 (Copy 阶段进行中)                               ║
║  │                                                                     ║
║  │ CONTAINER RELEASE:                                                  ║
║  │   释放所有 Map Container，资源回收到集群                            ║
║  └─────────────────────────────────────────────────────────────────────  ║
║                                                                         ║
║  [10:00:30.000] ──────────────────────────────────────────────────────  ║
║  │ EVENT: 所有 Reduce Task 完成                                       ║
║  │ STATUS:                                                             ║
║  │   Map: 9/9 完成 (100%)                                             ║
║  │   Reduce: 3/3 完成 (100%)                                          ║
║  │                                                                     ║
║  │ FINAL:                                                              ║
║  │   - 作业耗时: 30 秒                                                 ║
║  │   - 数据本地性: 100% NODE_LOCAL                                     ║
║  │   - 输出路径: /output/pv_uv_result/                                ║
║  │                                                                     ║
║  │ CONTAINER RELEASE:                                                  ║
║  │   释放所有 Reduce Container 和 AM Container                        ║
║  │   集群资源完全释放                                                  ║
║  └─────────────────────────────────────────────────────────────────────  ║
║                                                                         ║
╚═════════════════════════════════════════════════════════════════════════╝
```

### 3.3 调度决策要点总结

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    YARN 调度决策要点                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【决策1】AM 放在哪个节点？                                             │
│  ─────────────────────────                                              │
│  依据: 节点当前负载、资源可用性                                         │
│  本例: node-1 负载最低，选择 node-1                                    │
│                                                                         │
│  【决策2】Map Task 放在哪个节点？                                       │
│  ────────────────────────────                                           │
│  依据: 数据本地性优先级                                                 │
│    1. NODE_LOCAL: 数据就在本节点 → 首选                                │
│    2. RACK_LOCAL: 数据在同机架其他节点 → 次选                          │
│    3. ANY: 跨机架 → 最后选择                                           │
│  本例: 所有 Map 都实现了 NODE_LOCAL                                    │
│                                                                         │
│  【决策3】Reduce Task 放在哪个节点？                                    │
│  ──────────────────────────────                                         │
│  依据: Reduce 无数据本地性要求（要从所有 Map 拉数据）                   │
│    - 优先考虑负载均衡                                                   │
│    - 避免网络热点（不要都放一个节点）                                   │
│  本例: 均匀分布到 3 个节点                                             │
│                                                                         │
│  【决策4】何时启动 Reduce？                                             │
│  ───────────────────────                                                │
│  依据: slowstart 参数（默认 5%）                                        │
│  权衡:                                                                  │
│    - 太早: Reduce 空等，浪费资源                                       │
│    - 太晚: 延长整体执行时间                                             │
│  本例: 3 个 Map 完成后 (33% > 5%) 启动 Reduce                          │
│                                                                         │
│  【决策5】Container 大小怎么定？                                        │
│  ─────────────────────────                                              │
│  依据: 任务特性                                                         │
│    - Map: 通常内存需求小（处理固定大小的 Block）                       │
│    - Reduce: 可能需要更多内存（Shuffle 缓冲区、排序）                  │
│  本例: Map=1024MB, Reduce=2048MB                                       │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 四、迁移到 Spark/Flink：什么变了，什么没变

### 4.1 用 Spark 重写 PV/UV 案例

```python
# Hadoop MapReduce 版本 (伪代码)
# ─────────────────────────────────
class PVUVMapper:
    def map(self, key, value):
        # key: 行偏移量
        # value: 日志行
        url = parse_url(value)
        user_id = parse_user_id(value)
        emit(url, user_id)

class PVUVReducer:
    def reduce(self, key, values):
        # key: url
        # values: [user_id, user_id, ...]
        pv = len(values)
        uv = len(set(values))
        emit(key, (pv, uv))

# 提交作业
job.setMapperClass(PVUVMapper)
job.setReducerClass(PVUVReducer)
job.waitForCompletion()
```

```python
# Spark 版本
# ─────────────────────────────────
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("PV_UV").getOrCreate()

# 读取数据 (自动处理 Block 边界，对用户透明)
logs = spark.read.text("/data/logs/")

# 解析日志
parsed = logs.rdd.map(lambda row: parse_log(row.value))

# 计算 PV/UV (一行搞定！)
result = parsed \
    .map(lambda x: (x['url'], x['user_id'])) \
    .groupByKey() \
    .mapValues(lambda users: {'pv': len(list(users)), 
                              'uv': len(set(users))})

# 或者用 DataFrame API (更推荐)
from pyspark.sql.functions import count, countDistinct

result_df = parsed.toDF(['url', 'user_id']) \
    .groupBy('url') \
    .agg(
        count('*').alias('pv'),
        countDistinct('user_id').alias('uv')
    )

result_df.write.parquet("/output/pv_uv_result/")
```

### 4.2 本质变化对比

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    Hadoop vs Spark 本质变化                             │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【变化1】执行模型                                                      │
│  ════════════════                                                       │
│                                                                         │
│  Hadoop MapReduce:                                                      │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  Job → Task → Task Attempt                                      │   │
│  │                                                                 │   │
│  │  每个 Task 是独立的 JVM 进程                                    │   │
│  │  Task 之间完全通过磁盘交换数据                                  │   │
│  │  Map 必须全部完成，Reduce 才能开始真正计算                      │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  Spark:                                                                │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  Application → Job → Stage → Task                               │   │
│  │                                                                 │   │
│  │  Executor 是长期运行的 JVM 进程（复用）                         │   │
│  │  Task 在 Executor 内以线程方式运行                              │   │
│  │  同一 Stage 内的 Task 可以流水线执行（Pipeline）               │   │
│  │  中间结果可以缓存在内存中                                       │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│                                                                         │
│  【变化2】Shuffle 实现                                                  │
│  ════════════════════                                                   │
│                                                                         │
│  Hadoop:                                                               │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  Map 输出 → 磁盘 → 网络 → 磁盘 → Reduce 读取                   │   │
│  │                                                                 │   │
│  │  ❌ 必须落盘，没有例外                                          │   │
│  │  ❌ 排序是强制的（即使 Reducer 不需要）                         │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  Spark:                                                                │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  Shuffle Write → (内存/磁盘) → 网络 → Shuffle Read             │   │
│  │                                                                 │   │
│  │  ✓ 尽量使用内存                                                 │   │
│  │  ✓ 支持不排序的 Shuffle（Hash Shuffle，已优化）                │   │
│  │  ✓ 排序是按需的（Sort Shuffle 也更高效）                       │   │
│  │  ✓ 支持 Shuffle 数据压缩                                       │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│                                                                         │
│  【变化3】容错机制                                                      │
│  ════════════════                                                       │
│                                                                         │
│  Hadoop:                                                               │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  Task 失败 → 重新执行整个 Task                                  │   │
│  │  Map 输出在本地磁盘，Reduce 失败需重新拉取                      │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  Spark:                                                                │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  基于 RDD 血缘（Lineage）重算                                   │   │
│  │                                                                 │   │
│  │  Stage 失败 → 只重算丢失的 Partition                           │   │
│  │  可以设置 Checkpoint 截断 Lineage                              │   │
│  │  Shuffle 数据可以持久化以避免重算                               │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│                                                                         │
│  【变化4】编程模型                                                      │
│  ════════════════                                                       │
│                                                                         │
│  Hadoop:                                                               │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  只有 Map 和 Reduce 两种原语                                    │   │
│  │  复杂逻辑需要多个 Job 串联                                      │   │
│  │  手动管理 Job 之间的依赖                                        │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  Spark:                                                                │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  丰富的算子: map, filter, groupBy, join, window...             │   │
│  │  自动划分 Stage 和构建 DAG                                      │   │
│  │  惰性求值 + 优化器                                              │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 4.3 被保留的设计思想

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    Hadoop 设计思想在 Spark/Flink 中的传承               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  【保留1】数据本地性 (Data Locality)                                   │
│  ══════════════════════════════════                                     │
│                                                                         │
│  Hadoop 思想: "移动计算比移动数据便宜"                                  │
│                                                                         │
│  Spark 继承:                                                           │
│    - Task 调度仍然优先考虑 NODE_LOCAL                                  │
│    - PROCESS_LOCAL > NODE_LOCAL > RACK_LOCAL > ANY                     │
│    - 可配置等待本地性的超时时间                                         │
│                                                                         │
│  Flink 继承:                                                           │
│    - 读取数据时考虑 Split 的位置信息                                   │
│    - 有数据本地性调度策略                                               │
│                                                                         │
│                                                                         │
│  【保留2】分区 (Partitioning) 概念                                     │
│  ═══════════════════════════════                                        │
│                                                                         │
│  Hadoop 思想: 相同 Key 必须在同一个 Reducer                             │
│                                                                         │
│  Spark 继承:                                                           │
│    - RDD 的 Partitioner (HashPartitioner, RangePartitioner)           │
│    - DataFrame 的 partitionBy                                          │
│    - 分区数影响并行度                                                   │
│                                                                         │
│  Flink 继承:                                                           │
│    - KeyedStream 按 Key 分区                                           │
│    - Partitioner 策略 (Hash, Rebalance, Broadcast)                    │
│                                                                         │
│                                                                         │
│  【保留3】Shuffle 的核心理念                                           │
│  ════════════════════════════                                           │
│                                                                         │
│  Hadoop 思想: 重新分布数据以支持全局聚合                                │
│                                                                         │
│  Spark 继承:                                                           │
│    - 宽依赖触发 Stage 边界                                             │
│    - Shuffle Write / Shuffle Read                                      │
│    - 优化目标仍是减少 Shuffle                                          │
│                                                                         │
│  Flink 继承:                                                           │
│    - 网络 Exchange                                                     │
│    - Forward / Hash / Rebalance 等策略                                │
│                                                                         │
│                                                                         │
│  【保留4】容错通过重算                                                  │
│  ═════════════════════                                                  │
│                                                                         │
│  Hadoop 思想: 失败的 Task 重新执行                                      │
│                                                                         │
│  Spark 继承:                                                           │
│    - 通过 Lineage 重算丢失的 Partition                                │
│    - 比重新读取原始数据更高效                                           │
│                                                                         │
│  Flink 继承:                                                           │
│    - Checkpoint + 重放机制                                             │
│    - Exactly-Once 语义                                                 │
│                                                                         │
│                                                                         │
│  【保留5】资源管理解耦                                                  │
│  ══════════════════════                                                 │
│                                                                         │
│  Hadoop 思想: YARN 统一管理集群资源                                     │
│                                                                         │
│  Spark 继承:                                                           │
│    - 可以运行在 YARN 上                                                │
│    - 也支持 Standalone, Kubernetes, Mesos                             │
│                                                                         │
│  Flink 继承:                                                           │
│    - 可以运行在 YARN 上                                                │
│    - 也支持 Kubernetes, Standalone                                    │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 4.4 迁移检查清单

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    从 Hadoop 迁移到 Spark 的检查清单                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ✅ 直接迁移（几乎无需改动逻辑）                                        │
│  ──────────────────────────────────                                     │
│  □ 简单的 Map-Only 作业 → Spark map/filter                             │
│  □ WordCount 类聚合 → Spark reduceByKey                                │
│  □ 读写 HDFS → Spark 原生支持                                          │
│  □ 读写 Hive 表 → SparkSQL 原生支持                                    │
│                                                                         │
│  ⚠️ 需要调整（逻辑相似但实现不同）                                      │
│  ──────────────────────────────────                                     │
│  □ Combiner → 使用 combineByKey 或 aggregateByKey                      │
│  □ 自定义 Partitioner → 继承 org.apache.spark.Partitioner             │
│  □ 二次排序 → 使用 repartitionAndSortWithinPartitions                  │
│  □ 多 Job 级联 → 单个 Spark Application 内的多 Action                  │
│                                                                         │
│  🔄 需要重新设计（范式变化）                                            │
│  ──────────────────────────────────                                     │
│  □ 迭代算法（如 PageRank）→ Spark 的迭代支持更自然                     │
│  □ 流处理需求 → Spark Streaming 或迁移到 Flink                         │
│  □ 交互式查询 → SparkSQL + Catalyst 优化器                             │
│  □ 机器学习 → Spark MLlib                                              │
│                                                                         │
│  💡 性能优化新思路                                                      │
│  ──────────────────────────────────                                     │
│  □ 利用内存缓存（cache/persist）避免重复计算                           │
│  □ 使用广播变量替代 DistributedCache                                   │
│  □ 使用 DataFrame API 获得 Catalyst 优化                               │
│  □ 调整 spark.sql.shuffle.partitions 控制并行度                       │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 总结

本文档从四个维度深入剖析了 Hadoop 的核心机制：

1. **Map Task 视角**：理解单个任务的"所见所得"，建立微观直觉
2. **Shuffle 深挖**：理解 MapReduce 的性能瓶颈所在
3. **YARN 调度**：理解资源如何被分配和管理
4. **Spark 迁移**：理解哪些是"永恒的设计智慧"，哪些是"技术迭代"

掌握这些知识后，你将能够：
- 诊断 MapReduce 作业的性能问题
- 设计更高效的数据处理流程
- 平滑迁移到 Spark/Flink 等现代框架
- 在面试中展示对分布式系统的深刻理解
