# MiniSpark - æ•™å­¦ç”¨ Spark æ¨¡æ‹Ÿæ¡†æ¶

MiniSpark æ˜¯ä¸€ä¸ªçº¯ Python å®ç°çš„ Apache Spark æ ¸å¿ƒæŠ½è±¡æ¨¡æ‹Ÿå™¨ï¼Œä¸“ä¸ºæ•™å­¦ç›®çš„è®¾è®¡ã€‚å®ƒä¸ä¾èµ–çœŸå®çš„ Spark/JVM/é›†ç¾¤ï¼Œä½†å°½å¯èƒ½å¤åˆ» Spark çš„æ ¸å¿ƒæ¦‚å¿µå’Œæ‰§è¡Œæµç¨‹ã€‚

## è®¾è®¡ç›®æ ‡

**å¯è§‚æµ‹ + å¯è§£é‡Š**ï¼šæ¯æ¬¡æ‰§è¡Œ Actionï¼Œéƒ½è¾“å‡ºæ¸…æ™°çš„æ—¥å¿—ä¸ ASCII æ‰§è¡Œå›¾ï¼Œå¸®åŠ©ç†è§£ä» DAG åˆ°ç‰©ç†æ‰§è¡Œï¼ˆJob/Stage/Taskï¼‰çš„å®Œæ•´è¿‡ç¨‹ã€‚

## æ ¸å¿ƒæ¦‚å¿µ

### 1. RDD (Resilient Distributed Dataset)
- ä¸å¯å˜çš„åˆ†å¸ƒå¼æ•°æ®é›†
- æ”¯æŒ Transformationï¼ˆæƒ°æ€§ï¼‰å’Œ Actionï¼ˆè§¦å‘æ‰§è¡Œï¼‰
- è®°å½•è¡€ç»Ÿï¼ˆLineageï¼‰æ”¯æŒå®¹é”™é‡ç®—

### 2. DAG è°ƒåº¦
- Action è§¦å‘ Job
- Job æŒ‰ shuffle è¾¹ç•Œåˆ‡åˆ†ä¸º Stage
- Stage å†… Task æŒ‰åˆ†åŒºå¹¶è¡Œæ‰§è¡Œ

### 3. Shuffle
- å®½ä¾èµ–è§¦å‘ shuffle
- Map ç«¯å†™å…¥ï¼ŒReduce ç«¯è¯»å–
- ç›¸åŒ key çš„æ•°æ®èšé›†åˆ°åŒä¸€åˆ†åŒº

### 4. Cache/Persist
- ç¼“å­˜ RDD è®¡ç®—ç»“æœ
- åç»­ä½¿ç”¨å¯ç›´æ¥è¯»å–ï¼Œé¿å…é‡ç®—

### 5. å®¹é”™
- é€šè¿‡ Lineage é‡ç®—å¤±è´¥çš„åˆ†åŒº
- æ”¯æŒä»»åŠ¡é‡è¯•

## é¡¹ç›®ç»“æ„

```
sp_sim_cursor/
â”œâ”€â”€ minispark/
â”‚   â”œâ”€â”€ __init__.py      # åŒ…åˆå§‹åŒ–å’Œå¯¼å‡º
â”‚   â”œâ”€â”€ rdd.py           # RDD æ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ scheduler.py     # DAG è°ƒåº¦å™¨
â”‚   â”œâ”€â”€ cluster.py       # ClusterManager å’Œ Executor
â”‚   â”œâ”€â”€ shuffle.py       # Shuffle ç®¡ç†å™¨
â”‚   â”œâ”€â”€ logger.py        # ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿ
â”‚   â”œâ”€â”€ dag.py           # DAG å¯è§†åŒ–
â”‚   â””â”€â”€ context.py       # SparkContext
â”œâ”€â”€ demo.py              # æ¼”ç¤ºç¨‹åº
â””â”€â”€ README.md
```

## åŠŸèƒ½æ¸…å•

### RDD Transformationsï¼ˆæƒ°æ€§ï¼‰
| æ“ä½œ | ä¾èµ–ç±»å‹ | è¯´æ˜ |
|------|---------|------|
| `map(f)` | çª„ä¾èµ– | å¯¹æ¯ä¸ªå…ƒç´ åº”ç”¨å‡½æ•° |
| `filter(f)` | çª„ä¾èµ– | è¿‡æ»¤å…ƒç´  |
| `flatMap(f)` | çª„ä¾èµ– | æ˜ å°„å¹¶å±•å¹³ |
| `mapPartitions(f)` | çª„ä¾èµ– | åˆ†åŒºçº§åˆ«æ˜ å°„ |
| `union(other)` | çª„ä¾èµ– | åˆå¹¶ä¸¤ä¸ª RDD |
| `keyBy(f)` | çª„ä¾èµ– | ç”Ÿæˆ key-value å¯¹ |
| `reduceByKey(f)` | **å®½ä¾èµ–** | æŒ‰ key èšåˆï¼ˆè§¦å‘ shuffleï¼‰ |
| `groupByKey()` | **å®½ä¾èµ–** | æŒ‰ key åˆ†ç»„ï¼ˆè§¦å‘ shuffleï¼‰ |
| `join(other)` | **å®½ä¾èµ–** | è¿æ¥ä¸¤ä¸ª RDDï¼ˆè§¦å‘ shuffleï¼‰ |

### RDD Actionsï¼ˆè§¦å‘æ‰§è¡Œï¼‰
| æ“ä½œ | è¯´æ˜ |
|------|------|
| `collect()` | æ”¶é›†æ‰€æœ‰æ•°æ®åˆ° Driver |
| `count()` | è®¡ç®—å…ƒç´ æ€»æ•° |
| `take(n)` | è·å–å‰ n ä¸ªå…ƒç´  |
| `first()` | è·å–ç¬¬ä¸€ä¸ªå…ƒç´  |
| `reduce(f)` | èšåˆæ‰€æœ‰å…ƒç´  |
| `saveAsTextFile(path)` | ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶ |

### æŒä¹…åŒ–
| æ“ä½œ | è¯´æ˜ |
|------|------|
| `cache()` | ç¼“å­˜ RDDï¼ˆç­‰åŒäº persistï¼‰ |
| `persist()` | æŒä¹…åŒ– RDD |
| `unpersist()` | å–æ¶ˆæŒä¹…åŒ– |

## æ—¥å¿—äº‹ä»¶

ç»“æ„åŒ–æ—¥å¿—åŒ…å«ä»¥ä¸‹äº‹ä»¶ç±»å‹ï¼š

| äº‹ä»¶ | è¯´æ˜ |
|------|------|
| `DAG_BUILT` | DAG æ„å»ºå®Œæˆ |
| `ACTION_CALLED` | Action è¢«è°ƒç”¨ |
| `JOB_SUBMITTED` | Job æäº¤ |
| `STAGE_PLANNED` | Stage è§„åˆ’ |
| `STAGE_START/END` | Stage å¼€å§‹/ç»“æŸ |
| `TASK_START/END` | Task å¼€å§‹/ç»“æŸ |
| `SHUFFLE_WRITE` | Shuffle å†™å…¥ |
| `SHUFFLE_READ` | Shuffle è¯»å– |
| `CACHE_PUT` | å†™å…¥ç¼“å­˜ |
| `CACHE_HIT` | ç¼“å­˜å‘½ä¸­ |
| `LINEAGE_RECOMPUTE` | Lineage é‡ç®— |
| `TASK_FAILED/RETRY` | ä»»åŠ¡å¤±è´¥/é‡è¯• |

## å¿«é€Ÿå¼€å§‹

```python
from minispark import SparkContext

# åˆ›å»º SparkContext
sc = SparkContext("MyApp", num_executors=4)

# åˆ›å»º RDD
rdd = sc.parallelize([1, 2, 3, 4, 5])

# Transformationsï¼ˆæƒ°æ€§ï¼Œä¸è§¦å‘è®¡ç®—ï¼‰
result = rdd.map(lambda x: x * 2).filter(lambda x: x > 5)

# Actionï¼ˆè§¦å‘æ‰§è¡Œï¼‰
print(result.collect())  # [6, 8, 10]

sc.stop()
```

## è¿è¡Œæ¼”ç¤º

```bash
cd sp_sim_cursor
python demo.py
```

æ¼”ç¤ºåŒ…å«ä¸‰ä¸ª Pipelineï¼š
1. **è¯é¢‘ç»Ÿè®¡**ï¼šå±•ç¤ºçª„ä¾èµ– + å®½ä¾èµ– + cache
2. **å®¹é”™é‡ç®—**ï¼šå±•ç¤º Lineage æœºåˆ¶
3. **Join æ“ä½œ**ï¼šå±•ç¤ºå¤š shuffle åœºæ™¯

## ç¤ºä¾‹è¾“å‡º

### é€»è¾‘ DAGï¼ˆè¡€ç»Ÿå›¾ï¼‰

```
============================================================
  é€»è¾‘ DAG (æ‰§è¡Œ count å‰)
============================================================

RDD[6] reduceByKey(filter(map(flatMap(textFile(sample.txt)))))
  (partitions=4)
  |
  | (wide: reduceByKey)
  | <== SHUFFLE BOUNDARY (shuffle_1)
  |
RDD[5] filter(map(flatMap(textFile(sample.txt))))
  (partitions=4)
  |
  | (narrow: filter)
  |
RDD[4] map(flatMap(textFile(sample.txt)))
  (partitions=4)
  |
  | (narrow: map)
  |
RDD[3] flatMap(textFile(sample.txt))
  (partitions=4)
  |
  | (narrow: flatMap)
  |
RDD[2] textFile(sample.txt)
  (partitions=4)

============================================================
```

### ç‰©ç†æ‰§è¡Œè®¡åˆ’

```
============================================================
  ç‰©ç†æ‰§è¡Œè®¡åˆ’ - Job 1 (action=count)
============================================================

  STAGE 1 (ShuffleMap, RDD[5])
  --------------------------------------------------
    ä»»åŠ¡: T1[p0]  T2[p1]  T3[p2]  T4[p3]
    åˆ†åŒºæ•°: 4
    Shuffle: shuffle_1
         \
          --> SHUFFLE WRITE
         |
    SHUFFLE READ
         |
         v

  STAGE 2 (Result, RDD[6])
  --------------------------------------------------
    ä»»åŠ¡: T5[p0]  T6[p1]  T7[p2]  T8[p3]
    åˆ†åŒºæ•°: 4

  RESULT -> Driver
============================================================
```

### æ‰§è¡Œæ—¥å¿—

```
ğŸ¯ event=ACTION_CALLED | job_id=1 | rdd_id=6 | note=Action 'count' è¢«è°ƒç”¨ï¼Œè§¦å‘ Job 1
ğŸ“‹ event=JOB_SUBMITTED | job_id=1 | note=Job 1 å·²æäº¤ï¼ŒåŒ…å« 2 ä¸ª Stage
â–¶ï¸  event=STAGE_START | job_id=1 | stage_id=1 | note=Stage 1 å¼€å§‹æ‰§è¡Œï¼Œ4 ä¸ªåˆ†åŒº
âš™ï¸  event=TASK_START | executor_id=executor-1 | task_id=1 | partition_id=0
ğŸ“¤ event=SHUFFLE_WRITE | shuffle_id=1 | task_id=0 | output_records=42
âœ”ï¸  event=TASK_END | task_id=1 | duration_ms=12.34 | input_records=50 | output_records=42
...
ğŸ“¥ event=SHUFFLE_READ | shuffle_id=1 | partition_id=0 | input_records=38
ğŸ’¾ event=CACHE_PUT | rdd_id=6 | partition_id=0 | output_records=12
âœ… event=STAGE_END | stage_id=2 | note=Stage 2 æ‰§è¡Œå®Œæˆ
```

### æ‰§è¡Œæ‘˜è¦

```
============================================================
  JOB 1 æ‰§è¡Œæ‘˜è¦
============================================================
  Stage æ•°é‡:        2
  Task æ€»æ•°:         8
  Shuffle æ“ä½œ:      8 (å†™: 4, è¯»: 4)
  Cache å‘½ä¸­:        0
  Cache å†™å…¥:        4
  Lineage é‡ç®—:      0
  Task å¤±è´¥/é‡è¯•:    0
  æ€»è¾“å…¥è®°å½•:        242
  æ€»è¾“å‡ºè®°å½•:        18
  æ€»æ‰§è¡Œæ—¶é—´:        156.78 ms
============================================================
```

## Stage åˆ‡åˆ†ç­–ç•¥

MiniSpark çš„ Stage åˆ‡åˆ†ç­–ç•¥ä¸çœŸå® Spark ä¸€è‡´ï¼š

1. **é‡åˆ°å®½ä¾èµ–å°±åˆ‡åˆ†**ï¼šå®½ä¾èµ–ï¼ˆå¦‚ reduceByKeyã€groupByKeyã€joinï¼‰éœ€è¦ shuffleï¼Œäº§ç”Ÿ Stage è¾¹ç•Œ
2. **çª„ä¾èµ–æµæ°´çº¿æ‰§è¡Œ**ï¼šçª„ä¾èµ–ï¼ˆå¦‚ mapã€filterã€flatMapï¼‰å¯ä»¥åœ¨åŒä¸€ Stage å†…ä¸²è”æ‰§è¡Œ
3. **Stage é¡ºåºæ‰§è¡Œ**ï¼šçˆ¶ Stage å¿…é¡»åœ¨å­ Stage ä¹‹å‰å®Œæˆ
4. **Task å¹¶è¡Œæ‰§è¡Œ**ï¼šåŒä¸€ Stage å†…çš„ Task å¯ä»¥å¹¶è¡Œæ‰§è¡Œ

## ä¾èµ–

- Python 3.7+
- çº¯æ ‡å‡†åº“ï¼ˆæ— ç¬¬ä¸‰æ–¹ä¾èµ–ï¼‰

ä½¿ç”¨çš„æ ‡å‡†åº“ï¼š
- `dataclasses`: æ•°æ®ç±»å®šä¹‰
- `typing`: ç±»å‹æ³¨è§£
- `concurrent.futures`: çº¿ç¨‹æ± ï¼ˆæ¨¡æ‹Ÿ Executorï¼‰
- `threading`: çº¿ç¨‹åŒæ­¥
- `json`: åºåˆ—åŒ–
- `uuid`: å”¯ä¸€æ ‡è¯†
- `time`: è®¡æ—¶
- `os`: æ–‡ä»¶æ“ä½œ
- `enum`: æšä¸¾ç±»å‹

## æ¶æ„è¯´æ˜

### Driver/Executor æ¨¡æ‹Ÿ

```
+------------------+          +-------------------+
|     Driver       |          |  ClusterManager   |
|                  |  submit  |                   |
| - SparkContext   |--------->| - Executor Pool   |
| - DAGScheduler   |          | - Task Scheduling |
| - ç”¨æˆ·ä»£ç         |<---------|                   |
+------------------+  result  +-------------------+
                                     |
                                     | dispatch
                                     v
                    +-------+-------+-------+-------+
                    |  E0   |  E1   |  E2   |  E3   |
                    +-------+-------+-------+-------+
                         Executor Threads
```

- **Driver**: è¿è¡Œç”¨æˆ·ä»£ç ï¼Œåè°ƒè°ƒåº¦
- **ClusterManager**: ç®¡ç† Executor ç”Ÿå‘½å‘¨æœŸ
- **Executor**: æ‰§è¡Œå…·ä½“ä»»åŠ¡ï¼ˆç”¨çº¿ç¨‹æ¨¡æ‹Ÿï¼‰

### Shuffle æµç¨‹

```
Stage 1 (Map Side)              Stage 2 (Reduce Side)
+--------+                      +--------+
| Task 0 |--\                /--| Task 0 |
+--------+   \   Shuffle    /   +--------+
+--------+    \  Files     /    +--------+
| Task 1 |-----+=========+------| Task 1 |
+--------+    /           \     +--------+
+--------+   /             \    +--------+
| Task 2 |--/               \---| Task 2 |
+--------+                      +--------+
```

æ¯ä¸ª Map Task æŒ‰ key çš„ hash å°†æ•°æ®å†™å…¥å¯¹åº”çš„ Reduce åˆ†åŒºæ–‡ä»¶ã€‚
æ¯ä¸ª Reduce Task è¯»å–æ‰€æœ‰ Map Task ä¸ºå…¶åˆ†åŒºå†™å…¥çš„æ•°æ®ã€‚

## æ‰©å±•é˜…è¯»

- [Apache Spark å®˜æ–¹æ–‡æ¡£](https://spark.apache.org/docs/latest/)
- ã€ŠSpark å¿«é€Ÿå¤§æ•°æ®åˆ†æã€‹
- [RDD è®ºæ–‡](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)
